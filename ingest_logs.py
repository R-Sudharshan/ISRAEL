import os
import json
from ingestor import LogIngestor
from detection.engine import run_detection_pipeline, format_alert_object
from api.db import get_db_connection
import mysql.connector # Added for mysql.connector.Error

def ingest_direct(file_path):
    print(f"[*] Starting ingestion for {file_path}")
    if not os.path.exists(file_path):
        print(f"[!] Error: {file_path} not found.")
        return

    ingestor = LogIngestor()
    normalized_logs = ingestor.parse_log_file(file_path)
    
    conn = get_db_connection()
    cursor = conn.cursor()
    
    processed_count = 0
    alerts_generated = 0

    print(f"[*] Processing {len(normalized_logs)} logs...")
    if normalized_logs:
        print(f"DEBUG: First normalized log keys: {list(normalized_logs[0].keys())}")
        print(f"DEBUG: First normalized log content: {normalized_logs[0]}")
        
    for log in normalized_logs:
        # Pre-process: Restore timestamp from timestamp_iso if needed
        if 'timestamp' not in log and 'timestamp_iso' in log:
            log['timestamp'] = log['timestamp_iso']

        try:
            # 1. Store Normalized Log
            # Updated to handle Super-Set of 8+ Domains
            # Prepare SQL
            # Dynamic Columns for generic insertion

            allowed_cols = [
                "timestamp", "src_ip", "dst_ip", "src_port", "dst_port", "protocol", "service", "action", 
                "policyid", "sentbyte", "rcvdbyte", "duration", "user", "device_type", "level", "logid",
                "qname", "raw_log", "msg", "src_country", "dst_country", "log_type", "host", "direction",
                "auth_type", "auth_result", "failure_reason", "location", "process_name", "process_id",
                "parent_process", "command_line", "file_path", "hash", "integrity_level", 
                "http_method", "url", "status_code", "user_agent", "request_size", "response_size", "session_id",
                "client_ip", "asset_id", "hostname", "mac_address", "os", "os_version", "role", "criticality", "last_seen",
                "alert_name", "detection_engine", "action_taken", "confidence", "query", "query_type", "response", "rcode", 
                "ttl", "resolver", "cloud_provider", "account_id", "api_call", "resource", "region", "result", "ip_address"
            ]
            
            # Filter keys that exist in both log and allowed_cols
            cols = [k for k in log.keys() if k in allowed_cols]
            
            if not cols:
                print(f"DEBUG: Skipping log with no matching columns: {log}")
                continue

            placeholders = ", ".join(["%s"] * len(cols))
            sql_log = f"INSERT INTO logs ({', '.join(cols)}) VALUES ({placeholders})"
            
            # Create values tuple in same order as cols
            vals = []
            for c in cols:
                val = log.get(c)
                # Ensure empty strings are None for Integers if necessary, or just rely on DB casting
                vals.append(val)
                
            cursor.execute(sql_log, tuple(vals))
            print(f"DEBUG: Executed INSERT for {log.get('log_type')}")
            # print(f"DEBUG: SQL: {sql_log}")
            # print(f"DEBUG: VALS: {vals}")
            log_id = cursor.lastrowid

            # 2. Detect Anomalies
            detections = run_detection_pipeline(log)
            
            for d in detections:
                alert_data = format_alert_object(d, log, log_id)
                # 3. Store Alert
                sql_alert = "INSERT INTO alerts (severity, detection_type, src_ip, device, timestamp, raw_log_reference, mitre_tactic, mitre_technique) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)"
                cursor.execute(sql_alert, (alert_data['severity'], alert_data['detection_type'], alert_data['src_ip'], alert_data['device'], alert_data['timestamp'], log_id, alert_data['mitre_tactic'], alert_data['mitre_technique']))
                alerts_generated += 1
            
            processed_count += 1
        except Exception as e:
            print(f"[!] Error processing log: {e}")
            continue
    
    conn.commit()
    cursor.close()
    conn.close()
    
    print(f"[+] Ingestion complete: {processed_count} logs processed, {alerts_generated} alerts generated.")

if __name__ == "__main__":
    # Look for the JSON file generated by traffic_generator.py
    log_file = "simulated_fortigate_logs.json"
    ingest_direct(log_file)
